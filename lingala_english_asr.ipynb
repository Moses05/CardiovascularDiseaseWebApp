{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO46vvoBw5BPtH6zKfkBALl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moses05/CardiovascularDiseaseWebApp/blob/main/lingala_english_asr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents"
      ],
      "metadata": {
        "id": "aeeQkhp87zBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Moses05/lingala-english-asr.git"
      ],
      "metadata": {
        "id": "LjjI951mw2mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c4797b-c748-47a7-cfb7-fe490d3abac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lingala-english-asr'...\n",
            "remote: Enumerating objects: 2981, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 2981 (delta 0), reused 3 (delta 0), pack-reused 2975 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2981/2981), 432.52 MiB | 25.77 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Ca6tNzpFdDoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"lingala-english-asr/LRSC/lingala\"\n",
        "\n",
        "train_audio_path = f\"{base_path}/train/audio\"\n",
        "train_transcript_path = f\"{base_path}/train/transcript.txt\"\n",
        "\n",
        "valid_audio_path = f\"{base_path}/valid/audio\"\n",
        "valid_transcript_path = f\"{base_path}/valid/transcript.txt\""
      ],
      "metadata": {
        "id": "hlJTzldR_B_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_audio_path)"
      ],
      "metadata": {
        "id": "RbDcP4DH52cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wave\n",
        "\n",
        "def check_sample_rate(dir):\n",
        "\n",
        "  wrongFramerate = []\n",
        "\n",
        "  for wav in os.listdir(dir):\n",
        "    path = f\"{dir}/{wav}\"\n",
        "\n",
        "    if os.path.isfile(path):\n",
        "      with wave.open(path, \"rb\") as wav_file:\n",
        "        if wav_file.getframerate() != 16000:\n",
        "          wrongFramerate.append(wav_file)\n",
        "\n",
        "  return wrongFramerate\n",
        "\n",
        "\n",
        "train_wrongSample = check_sample_rate(train_audio_path)\n",
        "valid_wrongSample = check_sample_rate(valid_audio_path)\n",
        "\n",
        "print(f\"list of train audio files not 16000hz: {train_wrongSample}\")\n",
        "print(f\"list of valid audio files not 16000hz: {valid_wrongSample}\")"
      ],
      "metadata": {
        "id": "DWrbEOlS5cwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manifest_path = f\"{base_path}/manifest\"\n",
        "\n",
        "dict_txt = f\"{manifest_path}/dict.ltr.txt\"\n",
        "\n",
        "train_letter = f\"{manifest_path}/train.ltr\"\n",
        "train_tsv = f\"{manifest_path}/train.tsv\"\n",
        "train_word = f\"{manifest_path}/train.wrd\"\n",
        "\n",
        "valid_letter = f\"{manifest_path}/valid.ltr\"\n",
        "valid_tsv = f\"{manifest_path}/valid.tsv\"\n",
        "valid_word = f\"{manifest_path}/valid.wrd\""
      ],
      "metadata": {
        "id": "XUoR7NjgAEWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "AeuocfnpVusG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import os\n",
        "\n",
        "def load_manifest_data(tsv_file, ltr_file, audio_path):\n",
        "    data = {\n",
        "        \"path\": [],\n",
        "        \"duration\": [],\n",
        "        \"text\": [],\n",
        "    }\n",
        "\n",
        "    # Load the .tsv file\n",
        "    with open(tsv_file, 'r') as tsv_f:\n",
        "        lines = tsv_f.readlines()[1:]  # Skip header\n",
        "        # lines = tsv_f.readlines()[1:]  # Skip header\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "\n",
        "            # Ensure two columns path and duration\n",
        "            if len(parts) != 2:\n",
        "              print(f\"skipping malformed line: {line}\")\n",
        "              continue\n",
        "\n",
        "            path, duration = parts\n",
        "            full_path = os.path.join(audio_path, path) # prepend base path\n",
        "\n",
        "            data[\"path\"].append(full_path)\n",
        "            data[\"duration\"].append(int(duration) / 1000)\n",
        "\n",
        "    # Load the .ltr file for transcriptions\n",
        "    with open(ltr_file, 'r') as ltr_f:\n",
        "        transcriptions = ltr_f.readlines()\n",
        "        data[\"text\"] = [trans.strip() for trans in transcriptions]\n",
        "\n",
        "    return Dataset.from_dict(data)\n",
        "\n",
        "# Load the training and validation datasets\n",
        "train_dataset = load_manifest_data(train_tsv, train_letter, train_audio_path)\n",
        "valid_dataset = load_manifest_data(valid_tsv, valid_letter, valid_audio_path)"
      ],
      "metadata": {
        "id": "IitIhsMgNcS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "# Load the feature extractor and model\n",
        "# feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
        "# model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")"
      ],
      "metadata": {
        "id": "OSy5J92fWRX-",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_vocab(vocab_path):\n",
        "    char_to_index = {}\n",
        "    index_to_char = {}\n",
        "\n",
        "    with open(vocab_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            char, index = line.strip().split()\n",
        "            index = int(index)\n",
        "            char_to_index[char] = index\n",
        "            index_to_char[index] = char\n",
        "\n",
        "    # Add the <blank> token if not already present\n",
        "    if \"<blank>\" not in char_to_index:\n",
        "        char_to_index[\"<blank>\"] = len(char_to_index)\n",
        "        index_to_char[len(index_to_char)] = \"<blank>\"\n",
        "\n",
        "    return char_to_index, index_to_char\n",
        "\n",
        "char_to_index, index_to_char = load_vocab(dict_txt)\n",
        "vocab_size = len(char_to_index)\n",
        "\n",
        "char_to_index, index_to_char = load_vocab(dict_txt)\n",
        "vocab_size = len(char_to_index)\n",
        "\n",
        "print(f\"Loaded vocabulary with {vocab_size} characters\")"
      ],
      "metadata": {
        "id": "P58AYMG4YFpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_ids(text, char_to_index):\n",
        "    return [char_to_index[char] for char in text if char in char_to_index]\n",
        "\n",
        "def add_labels(batch):\n",
        "    # Convert text into numerical labels\n",
        "    batch[\"labels\"] = text_to_ids(batch[\"text\"], char_to_index)\n",
        "    return batch"
      ],
      "metadata": {
        "id": "1HRee6ocXKwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "\n",
        "def preprocess_audio(batch):\n",
        "  waveform, sample_rate = torchaudio.load(batch[\"path\"])\n",
        "\n",
        "  if sample_rate != 16000:\n",
        "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "    waveform = resampler(waveform)\n",
        "\n",
        "  waveform = waveform / torch.max(torch.abs(waveform))\n",
        "\n",
        "  inputs = feature_extractor(\n",
        "      waveform.squeeze().numpy(),\n",
        "      sampling_rate=16000,\n",
        "      return_tensors=\"pt\",\n",
        "      # padding=True,\n",
        "  )\n",
        "\n",
        "  batch[\"input_values\"] = inputs.input_values[0].numpy()\n",
        "  return batch\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_audio, remove_columns=[\"path\", \"duration\"])\n",
        "valid_dataset = valid_dataset.map(preprocess_audio, remove_columns=[\"path\", \"duration\"])"
      ],
      "metadata": {
        "id": "iyXJv0m4Y_np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset)\n",
        "print(valid_dataset)"
      ],
      "metadata": {
        "id": "UpS7fBtmBc8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add labels to the dataset\n",
        "train_dataset = train_dataset.map(add_labels, remove_columns=[\"text\"])\n",
        "valid_dataset = valid_dataset.map(add_labels, remove_columns=[\"text\"])\n",
        "\n",
        "print(train_dataset)\n",
        "print(valid_dataset)"
      ],
      "metadata": {
        "id": "Dtol7cGLXTP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_audio(sample, max_length=80000):\n",
        "  if len(sample[\"input_values\"]) > max_length:\n",
        "    sample[\"input_values\"] = sample[\"input_values\"][:max_length]\n",
        "  return sample\n",
        "\n",
        "train_dataset = train_dataset.map(truncate_audio)\n",
        "valid_dataset = valid_dataset.map(truncate_audio)"
      ],
      "metadata": {
        "id": "VH8g-FN8PjoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "  input_values = [torch.tensor(sample[\"input_values\"]) for sample in batch]\n",
        "  input_values_padded = pad_sequence(input_values, batch_first=True, padding_value=0.0)\n",
        "\n",
        "  labels = [torch.tensor(sample[\"labels\"]) for sample in batch]\n",
        "  labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "\n",
        "  return {\"input_values\": input_values_padded, \"labels\": labels_padded}"
      ],
      "metadata": {
        "id": "ZuBSpmDDr7cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=1, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "G8vKphpPq7jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HGfeXsmYWW2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    input_values = batch[\"input_values\"]\n",
        "    print(f\"Batch Shape:\", input_values.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "uYWIW2QIrUZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sQZX2kO8aK94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Y-Dufr2Yhoxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import Wav2Vec2Model\n",
        "\n",
        "class Wav2Vec2CTC(nn.Module):\n",
        "  def __init__(self, model, vocab_size):\n",
        "    super(Wav2Vec2CTC, self).__init__()\n",
        "    self.feature_extractor = model\n",
        "    self.ctc_head = nn.Linear(self.feature_extractor.config.hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, input_values):\n",
        "    features = self.feature_extractor(input_values).last_hidden_state\n",
        "\n",
        "    logits = self.ctc_head(features)\n",
        "    return logits\n",
        "\n",
        "wav2vec2_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
        "\n",
        "model = Wav2Vec2CTC(wav2vec2_model, vocab_size)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "Zl6uEJ5bLlDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "blank_index = char_to_index[\"<blank>\"]\n",
        "print(f\"Blank index: {blank_index}\")\n",
        "\n",
        "criterion = nn.CTCLoss(blank=blank_index, zero_infinity=True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "j5HV1Zk4RYdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "aDvIv5glcrgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            inputs = batch[\"input_values\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            # inputs = batch[\"input_values\"]\n",
        "            # labels = batch[\"labels\"]\n",
        "\n",
        "            # Forward pass through the model\n",
        "            logits = model(inputs)\n",
        "            logits = logits.log_softmax(2).permute(1, 0, 2)  # Shape: (seq_len, batch, vocab_size)\n",
        "\n",
        "            # Calculate input lengths based on model output\n",
        "            input_lengths = torch.full((logits.size(1),), logits.size(0), dtype=torch.long)\n",
        "            label_lengths = torch.sum(labels != -100, dim=1)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(logits, labels, input_lengths, label_lengths)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "# Run the training\n",
        "train_model(model, train_loader, criterion, optimizer)"
      ],
      "metadata": {
        "id": "ivjmAegpRwGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate jiwer"
      ],
      "metadata": {
        "id": "i5WUV8HC1ugo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HiBiJ9iZ5BPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "import torch\n",
        "\n",
        "wer_metric = load(\"wer\")\n",
        "\n",
        "def evaluate_model(model, valid_loader, index_to_char):\n",
        "    model.eval()\n",
        "    total_wer = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in valid_loader:\n",
        "        inputs = batch[\"input_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass through model to get logits\n",
        "        logits = model(inputs).log_softmax(2)\n",
        "\n",
        "        predicted_ids = torch.argmax(logits, dim=1)\n",
        "\n",
        "        pred_texts = [\"\".join([index_to_char[i] for i in pred if i in index_to_char]) for pred in predicted_ids]\n",
        "\n",
        "        label_texts = [\"\".join([index_to_char[i] for i in label if i in index_to_char]) for label in labels]\n",
        "\n",
        "        non_empty_indices = [i for i, label in enumerate(label_texts) if label]\n",
        "\n",
        "        if not non_empty_indices:\n",
        "          continue\n",
        "\n",
        "        pred_texts = [pred_texts[i] for i in non_empty_indices]\n",
        "        label_texts = [label_texts[i] for i in non_empty_indices]\n",
        "\n",
        "        wer = wer_metric.compute(predictions=pred_texts, references=label_texts)\n",
        "        total_wer += wer\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_wer = total_wer / num_batches if num_batches > 0 else float(\"inf\")\n",
        "    print(f\"Validation WER: {avg_wer:.4f}\")\n",
        "\n",
        "\n",
        "evaluate_model(model, valid_loader, index_to_char)"
      ],
      "metadata": {
        "id": "9bc0xo5aTSFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ujK3c6Tnbggz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}